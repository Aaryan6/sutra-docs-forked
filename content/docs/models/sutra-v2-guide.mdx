---
title: SUTRA-V2 Guide
---

Welcome to the Build with SUTRA-V2 guide ‚Äî designed to help developers harness the power of SUTRA-V2 for multilingual conversational AI, complex reasoning, and structured problem-solving.

### üîç What is SUTRA-V2?

SUTRA-V2 is our multilingual AI model designed for instruction execution and conversational intelligence across 50+ languages. Trained on a diverse blend of proprietary and open-access datasets, it excels in handling complex tasks with high accuracy. With deep proficiency across Latin, Indic, and Far Eastern languages, SUTRA-V2 delivers natural, context-aware responses, making it a powerful solution for global enterprises, multilingual assistants, and cross-language AI applications.

SUTRA-V2 is an advanced multilingual AI model optimized Multi-step logical inference, Structured problem-solving, Deep contextual analysis and High-performance conversational AI across 50+ languages. Following are some example scenarios where SUTRA-V2 is ideal choice.


| Scenario              | Example                                            |
| --------------------- | -------------------------------------------------- |
| Legal Analysis        | Interpret complex legal documents and policies     |
| Business Intelligence | Analyze market trends and strategic decisions      |
| Scientific Reasoning  | Evaluate cause-effect relationships in research    |
| Multilingual Q&A      | Answer queries in 50+ languages with high accuracy |


### Quickstart in Python

SUTRA-V2's API is fully compatible with OpenAI‚Äôs specifications, supporting seamless integration for developers. To begin using the SUTRA-V2 API, install the required package:

```python
# SUTRA models are OpenAI API compatible
!pip install -qU openai
```

Authenticate with your API key:

```python
from openai import OpenAI

client = OpenAI(
    base_url='https://api.two.ai/v2',
    api_key="YOUR_SUTRA_API_KEY"
)
```

Make a basic completion request:

```python
response = client.chat.completions.create(
    model='sutra-v2',
    messages=[
        {"role": "user", "content": "Tell me about artificial intelligence in 3 sentences."}
    ],
    max_tokens=1024,
    temperature=0.7
)

print(response.choices[0].message.content)
```

### Streaming Response Example

```python
stream = client.chat.completions.create(
    model='sutra-v2',
    messages=[
        {"role": "user", "content": "Write a short story about a robot who discovers emotions."}
    ],
    max_tokens=1024,
    temperature=0.7,
    stream=True
)

for chunk in stream:
    if chunk.choices and chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end='', flush=True)
```

### Multilingual Support Example

```python
def multilingual_response(prompt, lang):
    print(f"[{lang}] {prompt}")
    response = client.chat.completions.create(
        model='sutra-v2',
        messages=[{"role": "user", "content": prompt}],
        max_tokens=1024,
        temperature=0.7
    )
    print(response.choices[0].message.content)

multilingual_response("‡§®‡§Æ‡§∏‡•ç‡§§‡•á, ‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•à‡§Ç?", "Hindi")
multilingual_response("¬øQu√© es la inteligencia artificial?", "Spanish")
```


### Recommended Parameters

| Parameter          | Purpose                | Recommended for V2      |
| ------------------ | ---------------------- | ----------------------- |
| `temperature`      | Controls randomness    | 0.7 for balanced output |
| `max_tokens`       | Limits response length | 512‚Äì1024                |
| `presence_penalty` | Reduces repetition     | 0.0‚Äì0.6                 |

### API Access (cURL)

```bash
curl -X POST https://api.two.ai/v2/chat/completions \
-H "Authorization: Bearer $SUTRA_API_KEY" \
-H "Content-Type: application/json" \
-d '{
  "model": "sutra-v2",
  "messages": [
    {"role": "user", "content": "Explain how blockchain improves supply chain transparency."}
  ]
}'
```

---

### Troubleshooting

- **Invalid API Key**: Verify your API key and ensure it‚Äôs correctly set in your environment or Colab secret.
- **Rate Limit Exceeded**: Upgrade your plan or reduce request frequency.
- **Empty Response**: Increase `max_tokens` or refine your prompt for clarity.
